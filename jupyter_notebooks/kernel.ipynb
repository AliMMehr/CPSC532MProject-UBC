{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings', 'README.MD', 'sample_submission.csv', 'test.csv', 'train.csv']\n",
      "['GoogleNews-vectors-negative300', 'README.MD']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords as stp\n",
    "from textblob import TextBlob\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "import json\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir(\"../input/embeddings\"))\n",
    "print(os.listdir(\"../input/embeddings/GoogleNews-vectors-negative300\"))\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['README.MD']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../clean_data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    print(os.listdir(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "f8d0d229ac52f7132cd6f3852d71defeea7d7095",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stp.words('english'))\n",
    "punctuations= [\"\\\"\",\"(\",\")\",\"*\",\",\",\"-\",\"_\",\".\",\"~\",\"%\",\"^\",\"&\",\"!\",\"#\",'@'\n",
    "               \"=\",\"\\'\",\"\\\\\",\"+\",\"/\",\":\",\"[\",\"]\",\"«\",\"»\",\"،\",\"؛\",\"?\",\".\",\"…\",\"$\",\n",
    "               \"|\",\"{\",\"}\",\"٫\",\";\",\">\",\"<\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
    "\n",
    "def load_data(filename):\n",
    "\n",
    "    data = pd.read_csv('../input/%s' % filename  #, encoding='ISO-8859-1'\n",
    "                        , engine=\"python\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_google_vector():\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
    "    return model\n",
    "\n",
    "def tweet2v(list_words, model):\n",
    "    sentence_vec = []\n",
    "    if len(list_words)!=0:\n",
    "        for word in list_words:\n",
    "            if word in model:\n",
    "                sentence_vec.append(model[word].tolist())\n",
    "    return sentence_vec\n",
    "\n",
    "def tweets2tokens(tweet_text,model):\n",
    "    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
    "    words=[]\n",
    "    for token in tokens:\n",
    "        if token.startswith( 'http' ):\n",
    "            url=1\n",
    "        else:\n",
    "            url=0\n",
    "            if  '@' not in token and token in model and token not in stop_words and token != \"\" and token not in punctuations:\n",
    "            # if  '@' not in token and token not in stop_words and token != \"\" and token not in punctuations:\n",
    "                words.append(token)\n",
    "    return tokens,url\n",
    "\n",
    "def tweet_text2features(tweet_text,model):\n",
    "    tokens,url=tweets2tokens(tweet_text,model)\n",
    "    \n",
    "    features=[]\n",
    "    \n",
    "    sentence_vec=tweet2v(tokens,model)\n",
    "    list1=punctuationanalysis(tweet_text)\n",
    "    for item in list1:\n",
    "        features.append(item)\n",
    "    features.append(negativewordcount(tokens))\n",
    "    features.append(positivewordcount(tokens))\n",
    "    features.append(capitalratio(tweet_text))\n",
    "    features.append(contentlength(tokens))\n",
    "    features.append(sentimentscore(tweet_text))\n",
    "    list1=poscount(tweet_text)\n",
    "    for item in list1:\n",
    "        features.append(item)\n",
    "    features.append(url)\n",
    "    qfeatures={'word_vectors':sentence_vec,'additional_features':features}\n",
    "    return qfeatures\n",
    "\n",
    "def batch_of_items2json_files(q_batch,model,batch_number,run_id):\n",
    "    print('starting run:%s batch:%s' % (run_id,batch_number))\n",
    "    batch_clean_data={}\n",
    "    for index, sample in q_batch.iterrows():\n",
    "        tweet_text=sample['question_text']\n",
    "        qid=sample['qid']\n",
    "        target=sample['target']\n",
    "        qfeatures=tweet_text2features(tweet_text,model)\n",
    "        #print(qfeatures)\n",
    "        batch_clean_data[qid]={'qfeatures':qfeatures,'target':target}\n",
    "    \n",
    "    with open('../clean_data/%s-%s.json' % (run_id,batch_number), 'w') as fp:\n",
    "        json.dump(batch_clean_data, fp)\n",
    "        print('Done batch %s'% batch_number)\n",
    "\n",
    "#punctuations\n",
    "def punctuationanalysis(tweet_text):\n",
    "    hasqmark =sum(c =='?' for c in tweet_text)\n",
    "    hasemark =sum(c =='!' for c in tweet_text)\n",
    "    hasperiod=sum(c =='.' for c in tweet_text)\n",
    "    hasstar=sum(c =='*' for c in tweet_text)\n",
    "    number_punct=sum(c in punctuations for c in tweet_text)\n",
    "    return hasqmark,hasemark,hasperiod,hasstar,number_punct\n",
    "\n",
    "def negativewordcount(tokens):\n",
    "    count = 0\n",
    "    negativeFeel = ['dick','penis','god']\n",
    "    for negative in negativeFeel:\n",
    "        if negative in tokens:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def positivewordcount(tokens):\n",
    "    count = 0\n",
    "    positivewords = []\n",
    "    for pos in positivewords:\n",
    "        if pos in tokens:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def capitalratio(tweet_text):\n",
    "    uppers = [l for l in tweet_text if l.isupper()]\n",
    "    capitalratio = len(uppers) / len(tweet_text)\n",
    "    return capitalratio\n",
    "\n",
    "def contentlength(words):\n",
    "    wordcount = len(words)\n",
    "    return wordcount\n",
    "\n",
    "def sentimentscore(tweet_text):\n",
    "    analysis = TextBlob(tweet_text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "def poscount(tweet_text):\n",
    "    postag = []\n",
    "    poscount = {}\n",
    "    poscount['Noun']=0\n",
    "    poscount['Verb']=0\n",
    "    poscount['Adjective'] = 0\n",
    "    poscount['Pronoun']=0\n",
    "    poscount['Adverb']=0\n",
    "    Nouns = {'NN','NNS','NNP','NNPS'}\n",
    "    Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
    "    word_tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '', tweet_text))\n",
    "    postag = nltk.pos_tag(word_tokens)\n",
    "    for g1 in postag:\n",
    "     if g1[1] in Nouns:\n",
    "        poscount['Noun'] += 1\n",
    "     elif g1[1] in Verbs:\n",
    "         poscount['Verb']+= 1\n",
    "     elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
    "         poscount['Adjective']+=1\n",
    "     elif g1[1]=='PRP' or g1[1]=='PRON':\n",
    "         poscount['Pronoun']+=1\n",
    "     elif g1[1]=='ADV':\n",
    "         poscount['Adverb']+=1\n",
    "    return poscount.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_features_for_data(model,data,run_id):\n",
    "    batch_size=3\n",
    "    \n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    processes=[]\n",
    "    for batch in chunker(data,batch_size):\n",
    "        batch_of_items2json_files(batch,model,i,run_id)\n",
    "        #p=Process(target=batch_of_items2json_iles,args=(batch,model,i,run_id))\n",
    "        #p.start()\n",
    "        #processes.append(p)\n",
    "        if i>2:\n",
    "            break\n",
    "        i+=1\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "def load_test_data():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "37bad519ac12b454e10fa4bf70f60e43e3dfc950",
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['qid', 'question_text', 'target'], dtype='object')\n",
      "                    qid                                      question_text  \\\n",
      "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
      "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
      "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
      "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
      "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "             target\n",
      "count  1.306122e+06\n",
      "mean   6.187018e-02\n",
      "std    2.409197e-01\n",
      "min    0.000000e+00\n",
      "25%    0.000000e+00\n",
      "50%    0.000000e+00\n",
      "75%    0.000000e+00\n",
      "max    1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data =load_data('train.csv')\n",
    "print(data.columns)\n",
    "print(data.head())\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1216bb091c8b0928e6ad680f1576700f934eefed",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = load_google_vector()\n",
    "print(\"load_google_vector loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model['book']-model['books']-model['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b6cd6247f9770854a2189874eebd42eb2e91251",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_features_for_data(model,data,'ali')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
