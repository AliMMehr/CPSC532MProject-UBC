{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\ndevice = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49628b3d1884d71eeff80bc3bc011dc9f07de5cb"
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../input/embeddings/\"))",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['paragram_300_sl999', 'wiki-news-300d-1M', 'GoogleNews-vectors-negative300', 'glove.840B.300d']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "423e75c89b86cb93c339b1102ebf5529f5fc7e52"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, extra_feature_dim, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(hidden_dim+extra_feature_dim, tagset_size)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # Before we've done anything, we dont have any hidden state.\n        # Refer to the Pytorch documentation to see exactly\n        # why they have this dimensionality.\n        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n        return (torch.zeros(1, 1, self.hidden_dim, device=device),\n                torch.zeros(1, 1, self.hidden_dim, device=device))\n\n    def forward(self, sentence_vec,extra_features):\n        lstm_out, self.hidden = self.lstm(\n            sentence_vec.view(len(sentence_vec), 1, -1), self.hidden)        \n        tag_space = self.hidden2tag(torch.cat((self.hidden[0].view(1,-1),extra_features),dim=1))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "738b1d2cf79864f9da040ce4e8328c36f5879f36"
      },
      "cell_type": "code",
      "source": "model = LSTMTagger(300, 200, 16, 2)\nif device==torch.device(\"cuda:0\"):\n    model.cuda()\n\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9c69303b10f5c6486050b876f3439ae45d62824"
      },
      "cell_type": "code",
      "source": "def extract_features_and_target_from_sample(sample):\n    sentence_vec=sample['qfeatures']['word_vectors']\n    extra_features=sample['qfeatures']['additional_features']\n    target=sample['target']\n    tesnor_sentence_vec=torch.tensor(sentence_vec, device=device,dtype=torch.float32).view(len(sentence_vec),1,-1)\n    tensor_extra_feature=torch.tensor(extra_features, device=device,dtype=torch.float32).view(1,-1)\n    tensor_target=torch.tensor([target], device=device,dtype=torch.long)\n    return tesnor_sentence_vec,tensor_extra_feature,tensor_target",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e77afffd2a84a296c83db15ed4df47e754ffd40b"
      },
      "cell_type": "code",
      "source": "def train_on_q_set(training_data):\n    i=0\n    for qid in training_data:\n        # Step 1. Remember that Pytorch accumulates gradients.\n        # We need to clear them out before each instance\n        model.zero_grad()\n\n        # Also, we need to clear out the hidden state of the LSTM,\n        # detaching it from its history on the last instance.\n        model.hidden = model.init_hidden()\n\n        # Step 2. Get our inputs ready for the network, that is, turn them into\n        # Tensors of word indices.\n        sentence_vec,extra_features,targets=extract_features_and_target_from_sample(training_data[qid])\n\n        # Step 3. Run our forward pass.\n        tag_scores = model(sentence_vec,extra_features)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by\n        #  calling optimizer.step()\n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n        if(i % 1000)==0:\n            print('trained upto sample %s' % i)\n        i+=1",
      "execution_count": 52,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4a0b2186aa2002706fe9f9dea0e33cae8840884"
      },
      "cell_type": "code",
      "source": "def train_on_epochs(epoch_num=10):\n    for e in range(epoch_num):\n        \n        train_on_q_set(train_set)",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c98bcd1c7f3b7f741343b031fab478e769d0e3a0"
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nprint('start1')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gensim\nimport nltk\nimport re\nfrom nltk.corpus import stopwords as stp\nfrom textblob import TextBlob\nimport multiprocessing\nfrom multiprocessing import Process\nimport json\nimport urllib\nimport bz2\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/embeddings\"))\nprint(os.listdir(\"../input/embeddings/GoogleNews-vectors-negative300\"))\n\n\n\n# Any results you write to the current directory are saved as output.\n\ndef create_directory(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    print(os.listdir(\"../\"))\ncreate_directory(\"../clean_data/\")\nprint(os.listdir(\"../clean_data/\"))\n\nstop_words = set(stp.words('english'))\npunctuations= [\"\\\"\",\"(\",\")\",\"*\",\",\",\"-\",\"_\",\".\",\"~\",\"%\",\"^\",\"&\",\"!\",\"#\",'@'\n               \"=\",\"\\'\",\"\\\\\",\"+\",\"/\",\":\",\"[\",\"]\",\"«\",\"»\",\"،\",\"؛\",\"?\",\".\",\"…\",\"$\",\n               \"|\",\"{\",\"}\",\"٫\",\";\",\">\",\"<\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n\ndef load_data(filename):\n\n    data = pd.read_csv('../input/%s' % filename  #, encoding='ISO-8859-1'\n                        , engine=\"python\")\n\n    return data\n\ndef load_google_vector():\n    model = gensim.models.KeyedVectors.load_word2vec_format(\n        '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin',binary=True)\n    return model\n\ndef tweet2v(list_words, model):\n    sentence_vec = []\n    if len(list_words)!=0:\n        for word in list_words:\n            if word in model:\n                sentence_vec.append(model[word].tolist())\n    return sentence_vec\n\ndef tweets2tokens(tweet_text,model):\n    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n    words=[]\n    for token in tokens:\n        if token.startswith( 'http' ):\n            url=1\n        else:\n            url=0\n            if  '@' not in token and token in model and token not in stop_words and token != \"\" and token not in punctuations:\n            # if  '@' not in token and token not in stop_words and token != \"\" and token not in punctuations:\n                words.append(token)\n    return tokens,url\n\ndef tweet_text2features(tweet_text,model):\n    tokens,url=tweets2tokens(tweet_text,model)\n    \n    features=[]\n    \n    sentence_vec=tweet2v(tokens,model)\n    list1=punctuationanalysis(tweet_text)\n    for item in list1:\n        features.append(item)\n    features.append(negativewordcount(tokens))\n    features.append(positivewordcount(tokens))\n    features.append(capitalratio(tweet_text))\n    features.append(contentlength(tokens))\n    features.append(sentimentscore(tweet_text))\n    list1=poscount(tweet_text)\n    for item in list1:\n        features.append(item)\n    features.append(url)\n    qfeatures={'word_vectors':sentence_vec,'additional_features':features}\n    return qfeatures\n\ndef batch_of_items2json_files(q_batch,model,batch_number,run_id):\n    print('starting run:%s batch:%s' % (run_id,batch_number))\n    batch_clean_data={}\n    for index, sample in q_batch.iterrows():\n        tweet_text=sample['question_text']\n        qid=sample['qid']\n        target=sample['target']\n        qfeatures=tweet_text2features(tweet_text,model)\n        #print(qfeatures)\n        batch_clean_data[qid]={'qfeatures':qfeatures,'target':target}\n    \n    \n    print('Done batch %s'% batch_number)\n    return batch_clean_data    \n        \ndef batch_of_items2features_dict(q_batch,model,batch_number,run_id): # The same as batch_of_items2json_files but returns the data instead of storing in the json files\n    print('starting run:%s batch:%s' % (run_id,batch_number))\n    batch_clean_data={}\n    for index, sample in q_batch.iterrows():\n        tweet_text=sample['question_text']\n        qid=sample['qid']\n        target=sample['target']\n        qfeatures=tweet_text2features(tweet_text,model)\n        #print(qfeatures)\n        batch_clean_data[qid]={'qfeatures':qfeatures,'target':target}\n    \n    with open('../clean_data/%s-%s.json.tar.bz2' % (run_id,batch_number), 'wb') as fp:\n        s=json.dumps(batch_clean_data)\n        fp.write(bz2.compress(s.encode()))\n        print('Done batch %s'% batch_number)\n\n#punctuations\ndef punctuationanalysis(tweet_text):\n    hasqmark =sum(c =='?' for c in tweet_text)\n    hasemark =sum(c =='!' for c in tweet_text)\n    hasperiod=sum(c =='.' for c in tweet_text)\n    hasstar=sum(c =='*' for c in tweet_text)\n    number_punct=sum(c in punctuations for c in tweet_text)\n    return hasqmark,hasemark,hasperiod,hasstar,number_punct\n\ndef negativewordcount(tokens):\n    count = 0\n    negativeFeel = ['dick','penis','god']\n    for negative in negativeFeel:\n        if negative in tokens:\n            count += 1\n    return count\n\ndef positivewordcount(tokens):\n    count = 0\n    positivewords = []\n    for pos in positivewords:\n        if pos in tokens:\n            count += 1\n    return count\n\ndef capitalratio(tweet_text):\n    uppers = [l for l in tweet_text if l.isupper()]\n    capitalratio = len(uppers) / len(tweet_text)\n    return capitalratio\n\ndef contentlength(words):\n    wordcount = len(words)\n    return wordcount\n\ndef sentimentscore(tweet_text):\n    analysis = TextBlob(tweet_text)\n    return analysis.sentiment.polarity\n\ndef poscount(tweet_text):\n    postag = []\n    poscount = {}\n    poscount['Noun']=0\n    poscount['Verb']=0\n    poscount['Adjective'] = 0\n    poscount['Pronoun']=0\n    poscount['Adverb']=0\n    Nouns = {'NN','NNS','NNP','NNPS'}\n    Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n    word_tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '', tweet_text))\n    postag = nltk.pos_tag(word_tokens)\n    for g1 in postag:\n     if g1[1] in Nouns:\n        poscount['Noun'] += 1\n     elif g1[1] in Verbs:\n         poscount['Verb']+= 1\n     elif g1[1]=='ADJ'or g1[1]=='JJ':\n         poscount['Adjective']+=1\n     elif g1[1]=='PRP' or g1[1]=='PRON':\n         poscount['Pronoun']+=1\n     elif g1[1]=='ADV':\n         poscount['Adverb']+=1\n    return poscount.values()\n\ndef store_features_for_data(model,data,run_id,batch_size=10000,start=0,stop=1):\n    \n    def chunker(seq, size):\n        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n    \n    \n    i=0\n    processes=[]\n    for batch in chunker(data,batch_size):\n        if i>=start and i<stop:    \n            #batch_of_items2json_files(batch,model,i,run_id)\n            p=Process(target=batch_of_items2json_files,args=(batch,model,i,run_id))\n            p.start()\n            processes.append(p)\n        \n        i+=1\n    for p in processes:\n        p.join()\n\ndef get_features_for_data_and_train_on_every_batch(word2vec_model,data,run_id,batch_size=10000,start=0,stop=1):\n    \n    def chunker(seq, size):\n        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n    \n    \n    i=0\n    for batch in chunker(data,batch_size):\n        if i>=start and i<stop:    \n            q_set=batch_of_items2features_dict(batch,word2vec_model,i,run_id)\n            train_on_q_set(q_set)\n        \n        i+=1\n        \ndef load_test_data():\n    pass\n\n\n\n\n",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": "start1\n['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n['paragram_300_sl999', 'wiki-news-300d-1M', 'GoogleNews-vectors-negative300', 'glove.840B.300d']\n['GoogleNews-vectors-negative300.bin']\n['lib', 'config', 'clean_data', 'input', 'working']\n['ali-0.json.tar.bz2']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ded68b68f4dfd99b195ac8c18251f8b255e31b75"
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../clean_data/\"))",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['ali-0.json.tar.bz2']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a1bef42db0d50ab3236763b91280ef867f4dd47"
      },
      "cell_type": "code",
      "source": "print('start')\nmodel1 = load_google_vector()\nprint(\"load_google_vector loaded!\")\n\n\n",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "start\nload_google_vector loaded!\n[]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4911dbc8a8287e8648ed57a5cde9881b87c2ad8e"
      },
      "cell_type": "code",
      "source": "data =load_data('train.csv')\nprint(data.columns)\nprint(data.head())\n\n\n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Index(['qid', 'question_text', 'target'], dtype='object')\n                    qid  ...   target\n0  00002165364db923c7e6  ...        0\n1  000032939017120e6e44  ...        0\n2  0000412ca6e4628ce2cf  ...        0\n3  000042bf85aa498cd78e  ...        0\n4  0000455dfa3e01eae3af  ...        0\n\n[5 rows x 3 columns]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b0b6f44c73237dddb4fdbcf5e063734715b6829"
      },
      "cell_type": "code",
      "source": "store_features_for_data(model1,data,'ali')",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "starting run:ali batch:0\nDone batch 0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a2bfc376793a124a94b0b6649566f131469418a"
      },
      "cell_type": "code",
      "source": "def unzip_q_set(run_id,batch_number):\n    with open('../clean_data/%s-%s.json.tar.bz2' % (run_id,batch_number),'rb') as fb:\n        s0=fb.read()\n        s1=bz2.decompress(s0)\n        print(type(s1))\n        q_set=json.loads(s1.decode())\n        return q_set",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94937d2b6f899bb6e2b5c89e17025b8f42a0c271",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "q_set=unzip_q_set('ali',0)",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-d6c75f5e3cff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munzip_q_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ali'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-b3c93b49b129>\u001b[0m in \u001b[0;36munzip_q_set\u001b[0;34m(run_id, batch_number)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../clean_data/%s-%s.json.tar.bz2'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0ms0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0ms1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mq_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/bz2.py\u001b[0m in \u001b[0;36mdecompress\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mdecomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBZ2Decompressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4967ef2e2d10ed8e7f1824a2b2e6a5d5f2df4d2a"
      },
      "cell_type": "code",
      "source": "print(len(q_set['00002165364db923c7e6']['qfeatures']['word_vectors']))\nfor t in q_set['00002165364db923c7e6']['qfeatures']:\n    print(t)",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": "11\nword_vectors\nadditional_features\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f11c2d4a35b2334e5287ad912b5179c64069835",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#train_on_q_set(q_set)\nget_features_for_data_and_train_on_every_batch()",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": "trained upto sample 0\ntrained upto sample 1000\ntrained upto sample 2000\ntrained upto sample 3000\ntrained upto sample 4000\ntrained upto sample 5000\ntrained upto sample 6000\ntrained upto sample 7000\ntrained upto sample 8000\ntrained upto sample 9000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44dd5db4f18b25fe315df1381475f4b639a5b179"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 59,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e610e5b91868ec37276e430c6629dd3b01502ad"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09c11937b070c392dda6bce45d6116389cf2a5ea"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}